{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When searching for data science opportunities after the conclusion of my master's program, I naturally started with Indeed.com, a popular American job searching website, but I immediately noticed a problem when inputting my search queries. Whenever I searched for jobs related to data science and machine learning, a number of unrelated jobs would come up, such as \"biochemical engineer\", \"tech support assistant\", and even \"sales representative\" to name a few, all of which are jobs that I am not quite interested in. At one point in time, I had to navigate through 3 pages of jobs before I even saw my first data science job and even that wasn't quite what I was looking for, as the company was looking for a Senior Data Scientist... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "In order to try to make my life easier, I decided to make use of what I learned just a couple weeks ago and try to create my own data scraper in order to display jobs that would help during the application process. The final output of my program would then output a .json and .csv file containing all of the jobs that are relevant to a young data science professional looking to enter the industry, such as myself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries that will be used during this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import requests\n",
    "from urllib.parse import urljoin \n",
    "from bs4 import BeautifulSoup, Tag \n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pyexcel as pe\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the base URL, create a request to retrieve data from the base URL, and create an HTML parser using BeautifulSoup. During this project, I will be scraping jobs located in San Francisco, California and will sort these jobs by the date that they were posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = \"https://www.indeed.com/jobs?q=data+scientist&l=San+Francisco&sort=date\"\n",
    "req = requests.get(base_url)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking into the HTML code of the base URL, obtain the main table that contains all of the listed jobs that are of interest. This ignores all other jobs, links, and text that are irrelevant to what we want. Using our HTML parser, we find the table in which the ID is 'resultsCol' and call this block of HTML code \"main\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main = soup.find('td', {'id' : 'resultsCol'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For only a single page on Indeed, the output of this function is a list, named 'job_list', in which each observation within this list is a dictionary entry that displays the name of the company, the title of the job, the location, how long ago the job was posted, and a link to the URL. \n",
    "\n",
    "To better explain the code, comments can be seen within the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSinglePage(main = main):\n",
    "    \n",
    "    # Specify the job title that is wanted\n",
    "    regex_job_title = \"[Dd]ata [Ss]cien\"\n",
    "    \n",
    "    \n",
    "    # All of the constraints that we use on our query.\n",
    "    # Note that since we do NOT want jobs with these names (Senior, Manager, Lead, etc.), the regex contains a '^'\n",
    "    regex_constraint1 = \"^((?![Ss]enior).)*$\"\n",
    "    regex_constraint2 = \"^((?![Mm]anage).)*$\"\n",
    "    regex_constraint3 = \"^((?![Pp][Hh]d).)*$\"\n",
    "    regex_constraint4 = \"^((?![Ll]ead).)*$\"\n",
    "    regex_constraint5 = \"^((?![Dd]irector).)*$\"\n",
    "    regex_constraint6 = \"^((?![Pp]rincipal).)*$\"\n",
    "    regex_constraint7 = \"^((?![Ss][Rr].).)*$\"\n",
    "    regex_constraint8 = \"^((?![Cc]hief).)*$\"\n",
    "    \n",
    "    # Put all of the constraints into a list\n",
    "    regex_constraints = [regex_constraint1, regex_constraint2, regex_constraint3, regex_constraint4,\n",
    "        regex_constraint5, regex_constraint6, regex_constraint7, regex_constraint8]\n",
    "    \n",
    "    # Initialize a list that will eventually contain of all of the jobs for this specific page\n",
    "    job_list = []\n",
    "\n",
    "    # Iterate through all of the jobs using the HTML parser and regular expressions\n",
    "    # Each of the 10 jobs listed on the page have a class of 'row result'\n",
    "    # The name of the company, the location, title, and age are all extracted from the HTML\n",
    "    for jobs in main.find_all('div', {'class': re.compile(r\"row result\")}):\n",
    "        company = jobs.find('span', {'class': 'company'}).text.strip()\n",
    "        location = jobs.find('span', {'class': 'location'}).text.strip()\n",
    "        title = jobs.find('h2', {'class': 'jobtitle'}).text.strip()\n",
    "        age = jobs.find('span', {'class': 'date'}).text.strip()\n",
    "        \n",
    "\n",
    "        # This if statement takes care of the case in which the same company posts multiple jobs\n",
    "        if company in job_list:\n",
    "            company = company + \"*\"\n",
    "            \n",
    "        # This variable will be used in the following if statement in order to count how many constraints this specific job title matches\n",
    "        constraint_counter = 0\n",
    "        \n",
    "        # This if statement initially looks to see if its title actually matches the job title that we are looking for\n",
    "        # If the title of the job matches the job that we are looking for (i.e Data Scientist), it goes into the following for-loop\n",
    "        if re.search(regex_job_title, title):\n",
    "            \n",
    "            # The for-loop iterates through the constrain list.\n",
    "            # If the title doesn't contain that specific constraint, it adds 1 to the constraint counter \n",
    "            for i in range(len(regex_constraints)):\n",
    "                if re.search(regex_constraints[i], title):\n",
    "                    constraint_counter += 1\n",
    "                    \n",
    "        # Checks if the constraint counter satisfies all constraints (i.e. does not contain 'Manager')\n",
    "        # If true, it gets the URL and appends the company name, title, location, age, and URL to the job_list\n",
    "        if constraint_counter == len(regex_constraints):\n",
    "            url = \"https://www.indeed.com\" + str(jobs.find('h2', {'class': 'jobtitle'}).a['href'])\n",
    "            job_list.append({'company': company, 'title': title, 'location': location, 'age': age, 'url': url})\n",
    "            \n",
    "            \n",
    "            \n",
    "    return(job_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first page (and as talked about in the Introduction section), no data science jobs were found that matches what we're looking for... An example of this can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSinglePage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually display and exemplify a page that contains what we are looking for, I'll output the 4th page by changing the base URL. From there, I need to re-initialize our request, the HTML parser, and the main block of text. Note that this is only done to exemplify the above code. We will be re-initializing everything back to its original code immediately afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'age': '1 day ago',\n",
       "  'company': 'Electronic Arts',\n",
       "  'location': 'Redwood City, CA 94065',\n",
       "  'title': 'Research Data Scientist',\n",
       "  'url': 'https://www.indeed.com/rc/clk?jk=b6cc4e352a182b68&fccid=617d7f961cfcf54a&vjs=3'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change base URL to the 4th page in order to show results.\n",
    "base_url = \"https://www.indeed.com/jobs?q=data+scientist&l=San+Francisco&sort=date&fromage=last&start=40\"\n",
    "req = requests.get(base_url)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "main = soup.find('td', {'id' : 'resultsCol'})\n",
    "\n",
    "getSinglePage(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen what an observation looks like, I am now going to re-initialize everything back to how it was originally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the base URL back to the original position\n",
    "base_url = \"https://www.indeed.com/jobs?q=data+scientist&l=San+Francisco&sort=date\"\n",
    "req = requests.get(base_url)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "main = soup.find('td', {'id' : 'resultsCol'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous function, we are now going to iterate through the specified number of pages in order to output the list 'allJobs'. This list will contain all jobs matching our criteria for the specified number of pages. At every iteration, a new request, HTML parser and main table needs to be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMultiplePages(numPages = 10):\n",
    "    original_url = base_url + \"&start=\"\n",
    "    allJobs = []\n",
    "    for i in range(0, numPages):\n",
    "        newPage = original_url + str(i * 10)\n",
    "        newReq = requests.get(newPage)\n",
    "        newSoup = BeautifulSoup(newReq.text, \"html.parser\")\n",
    "        newPage_main = newSoup.find('td', {'id' : 'resultsCol'})\n",
    "        list = getSinglePage(newPage_main)\n",
    "        for i in list:\n",
    "            allJobs.append(i)\n",
    "\n",
    "    return(allJobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the types of data science jobs that we are specifically looking for in the first 15 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMultiplePages(numPages = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ability to scrape multiple Indeed webpages, I then created a function necessary to write this to a .JSON file. As an input, the function takes in the list that contains every job (which can be generated in the getMultiplePages() function) and the name of the .JSON file that you want to write to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createJSON(job_list, output_name):\n",
    "    with open(output_name, 'w') as f:\n",
    "        json.dump(job_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PyExcel library and the previously created .JSON file, this function then creates a .CSV file that can be opened in Excel. When opening the .CSV file, the names of the columns will be \"Company Name\", \"Job Title\", \"Location\", \"Data Posted\", and \"URL\". The inputs necessary to run this function is the name of the previously created .JSON file and the name of the .CSV file that you would like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createCSV(json_file, output_file):\n",
    "    with open(json_file) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        json_data.close()\n",
    "    allJobs = []\n",
    "    headers = ['Company Name', 'Job Title', 'Location', 'Date Posted', 'URL']\n",
    "    allJobs.append(headers)\n",
    "    for i in range(len(d)):\n",
    "        newJob = []\n",
    "        newJob.append(d[i]['company'])\n",
    "        newJob.append(d[i]['title'])\n",
    "        newJob.append(d[i]['location'])\n",
    "        newJob.append(d[i]['age'])\n",
    "        newJob.append(d[i]['url'])\n",
    "        allJobs.append(newJob)\n",
    "\n",
    "    sheet = pe.Sheet(allJobs)\n",
    "    sheet.save_as(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this can be used multiple times, I found that it might be helpful to also include the date at which these data scrapes took place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()\n",
    "month_day_year = str(date.month) + \"_\" + str(date.day) + \"_\" + str(date.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allJobs = getAllJobTitles(30)\n",
    "createJSON(allJobs, \"Indeed_jobs_\" + month_day_year + \".json\")\n",
    "createCSV(\"Indeed_jobs_\" + month_day_year + \".json\", \"Indeed_jobs_\" + month_day_year + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! Check your directory now for both a .JSON file and a .CSV file that lists all of the data science jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In conclusion, I thought that this personal project really helped cement what I was taught during my Applied Data Mining (Natural Language Processing) lecture just a few weeks ago. The final output is something that I think is very usable in the near future and could help simplify the job application process for me. In the future, I hope to come back to this code and find a better way to deal with the constraints that were specified using regular expressions. Overall, however, I'm very pleased with my results and hope you are too!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
